{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wrong-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "private-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of col_names in list must be equal number of rows df\n",
    "def transform_columns_to_rows(df: pd.DataFrame, col_names: list):\n",
    "    pd_series = pd.DataFrame()\n",
    "    for i, col in zip(range(df.shape[0]), col_names):\n",
    "        pd_series[col] = df.stack()[i]\n",
    "    return pd_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinated-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(list_obj: list):\n",
    "    now = datetime.datetime.now()\n",
    "    dates = []\n",
    "    for obj in list_obj:\n",
    "        if re.search(r'[a-zA-Z]', obj):\n",
    "            date = ''.join(re.findall(r'[a-zA-Z]+', obj))\n",
    "        elif (len(obj) == 12) & (re.search(r'[a-zA-Z]', obj) is None):\n",
    "            date = obj.replace(',', '.' + str(now.year))\n",
    "        else: \n",
    "            date = obj[:6] + str(now.year)[:2] + obj[6:8] + obj[9:15]\n",
    "        dates.append(date)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occasional-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_slice_list(list_obj: list, septener: str):\n",
    "    k = 2\n",
    "    even = list_obj[k-1::k]\n",
    "    odd = list_obj[k-2::k]\n",
    "    all_res = []\n",
    "    for x, y in zip(odd, even):\n",
    "        res = x + septener + y\n",
    "        all_res.append(res)\n",
    "    return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blessed-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_part_of_string(str_obj: str, start_board: str, end_board: str):\n",
    "    reg_str =  start_board +'(.*?)' + end_board \n",
    "    return re.findall(reg_str, str_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chubby-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_html(str_url: str): \n",
    "    html = requests.get(str_url).content\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    all_matches_db = {}\n",
    "    for each_tb in soup.find_all('div', {'class': 'live_comptt_bd'}):\n",
    "        ligue_header = each_tb.find('div', {'class': 'block_header'}).get_text()\n",
    "        ligue_header = ''.join(re.findall(r'\\n (.*?)\\n', ligue_header))  \n",
    "        ligue_header =   'Friendly' if ligue_header == '' else ligue_header\n",
    "        \n",
    "        season_id = ''.join(cut_part_of_string(str(each_tb),'season_id=', '\\''))\n",
    "        if (season_id == '') & (ligue_header != 'Friendly'):\n",
    "            season_id = 'Cup' \n",
    "        elif ligue_header == 'Friendly': \n",
    "            season_id = 'Friendly'\n",
    "        \n",
    "        comp_id = each_tb.get('id')[3:] \n",
    "        \n",
    "        game_ids = [x.get('dt-id') for x in each_tb.find_all('a', {'class': 'game_link'})]\n",
    "        \n",
    "        game_titles = [x.get('title') for x in each_tb.find_all('a', {'class': 'game_link'})]\n",
    "        \n",
    "        game_times_utc = [x.get_text() for x in each_tb.find_all('span', {'class': 'size10'})]\n",
    "        game_times_utc = transform_date(game_times_utc)\n",
    "        \n",
    "        game_statuses = [x if re.search('[a-zA-Z]', str(x)) else 'Finished' for x in game_times_utc]\n",
    "        \n",
    "        game_times_utc = [x[:16] for x in game_times_utc if re.search(r'[a-zA-Z]?', str(x))]\n",
    "        \n",
    "        all_goals = [x.get_text() for x in each_tb.find_all('div', {'class': 'gls'})]\n",
    "        all_goals = double_slice_list(all_goals, ':')\n",
    "\n",
    "        stages = [x.get_text() for x in each_tb.find_all('div', {'class': 'stage'})]\n",
    "        \n",
    "        matches = dict()\n",
    "        for game_id, game_utc, game_title, goals, game_status in zip(game_ids, game_times_utc, game_titles, all_goals, game_statuses): \n",
    "            matches.update({game_id: [ligue_header, comp_id, season_id, game_utc, game_title, goals, game_status]})\n",
    "            \n",
    "        all_matches_db.update(matches)\n",
    "        \n",
    "    return all_matches_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weekly-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-11-2013 last day with bet's data on soccer365, 2697 - days before now\n",
    "def create_date_list(numdays: int, start_year: int, start_month: int, start_day: int):\n",
    "    base = datetime.date(start_year, start_month, start_day)\n",
    "    date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optimum-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function take url without date\n",
    "def parsing_write_by_date(numdays: int, start_year: int, start_month: int, start_day: int, start_url='https://soccer365.me/online/&date='):\n",
    "    date_list = create_date_list(numdays, start_year, start_month, start_day)\n",
    "    all_matches_db = {}\n",
    "    for date in date_list:\n",
    "        print(date)\n",
    "        main_url = start_url + str(date)\n",
    "        matches_db = scrape_html(main_url)\n",
    "        all_matches_db.update(matches_db)\n",
    "        time.sleep(3)\n",
    "    all_matches = open('all_matches', 'wb')\n",
    "    pickle.dump(all_matches_db, all_matches)  \n",
    "    all_matches.close()\n",
    "    return print('Data is saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vertical-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing_write_by_date(numdays=5, start_year=2021, start_month=3, start_day=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "still-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('all_matches', 'rb') as f:\n",
    "#     all_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compressed-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_transform = pd.DataFrame(scrape_html('https://soccer365.me/online/&date=2020-03-20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "altered-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_columns_to_rows(df_to_transform, ['ligue_header', 'comp_id', 'season_id', 'game_utc', 'game_title','goals', 'game_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "religious-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index':'game_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "artistic-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
